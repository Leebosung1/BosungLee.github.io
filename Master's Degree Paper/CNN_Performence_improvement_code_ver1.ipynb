{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true,"mount_file_id":"1At9Kg75QjP8W4D2Vy6cT4hft5tzUSrQl","authorship_tag":"ABX9TyNbtmdMEdbdqHq+9ZJvaAGP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EqBqn069nCrb"},"outputs":[],"source":[]},{"cell_type":"code","source":["# specify path to data  \n","path2data = '/content/drive/MyDrive/EmptyShelf_Detection/Data'"],"metadata":{"id":"-49F-QtNnfbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습된 모델 load (weight 값 포함)\n","#from efficientnet_pytorch import EfficientNet\n","#model = EfficientNet.from_pretrained('efficientnet-b0')"],"metadata":{"id":"W5wIWHlnszTh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import package\n","\n","# model\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from torch import optim\n","\n","# dataset and transformation\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","import os\n","\n","# display images\n","from torchvision import utils\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# utils\n","import numpy as np\n","from torchsummary import summary\n","import time\n","import copy\n","import random\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"AhUbT-1Itnwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","## 데이타 로드\n","batch_size  = 32\n","random_seed = 555\n","random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","\n","## make dataset\n","from torchvision import transforms, datasets\n","data_path = '/content/drive/MyDrive/EmptyShelf_Detection/Data'  # class 별 폴더로 나누어진걸 확 가져와서 라벨도 달아준다\n","president_dataset = datasets.ImageFolder(\n","                                data_path,\n","                                transforms.Compose([\n","                                    transforms.Resize((224, 224)),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","                                ]))\n","## data split\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Subset\n","train_idx, tmp_idx = train_test_split(list(range(len(president_dataset))), test_size=0.2, random_state=random_seed)\n","datasets = {}\n","datasets['train'] = Subset(president_dataset, train_idx)\n","tmp_dataset       = Subset(president_dataset, tmp_idx)\n","\n","val_idx, test_idx = train_test_split(list(range(len(tmp_dataset))), test_size=0.5, random_state=random_seed)\n","datasets['valid'] = Subset(tmp_dataset, val_idx)\n","datasets['test']  = Subset(tmp_dataset, test_idx)\n","\n","## data loader 선언\n","dataloaders, batch_num = {}, {}\n","dataloaders['train'] = torch.utils.data.DataLoader(datasets['train'],\n","                                              batch_size=batch_size, shuffle=True,\n","                                              num_workers=4)\n","dataloaders['valid'] = torch.utils.data.DataLoader(datasets['valid'],\n","                                              batch_size=batch_size, shuffle=False,\n","                                              num_workers=4)\n","dataloaders['test']  = torch.utils.data.DataLoader(datasets['test'],\n","                                              batch_size=batch_size, shuffle=False,\n","                                              num_workers=4)\n","batch_num['train'], batch_num['valid'], batch_num['test'] = len(dataloaders['train']), len(dataloaders['valid']), len(dataloaders['test'])\n","print('batch_size : %d,  tvt : %d / %d / %d' % (batch_size, batch_num['train'], batch_num['valid'], batch_num['test']))\n"],"metadata":{"id":"QNpLJMIgtny0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","if not os.path.exists(path2data):\n","    os.mkdir(path2data)\n","\n","# load dataset\n","train_ds = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n","val_ds = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n","\n","print(len(train_ds))\n","print(len(val_ds))\n","'''"],"metadata":{"id":"adEBiLPXoMZk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 데이타 체크\n","import torchvision\n","def imshow(inp, title=None):\n","    \"\"\"Imshow for Tensor.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","num_show_img = 5\n","\n","class_names = {\n","    \"0\": \"Empty\",      # \"0\": \"Empty\"\n","    \"1\": \"NotEmpty\",   # \"1\": \"NotEmpty\"\n","\n","}\n","\n","# train check\n","inputs, classes = next(iter(dataloaders['train']))\n","out = torchvision.utils.make_grid(inputs[:num_show_img])  # batch의 이미지를 오려부친다\n","imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])\n","# valid check\n","inputs, classes = next(iter(dataloaders['valid']))\n","out = torchvision.utils.make_grid(inputs[:num_show_img])  # batch의 이미지를 오려부친다\n","imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])\n","# test check\n","inputs, classes = next(iter(dataloaders['test']))\n","out = torchvision.utils.make_grid(inputs[:num_show_img])  # batch의 이미지를 오려부친다\n","imshow(out, title=[class_names[str(int(x))] for x in classes[:num_show_img]])"],"metadata":{"id":"NDWf1XJJtn0o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Swish activation function\n","class Swish(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        return x * self.sigmoid(x)\n","\n","# check\n","if __name__ == '__main__':\n","    x = torch.randn(3, 3, 224, 224)\n","    model = Swish()\n","    output = model(x)\n","    print('output size:', output.size())"],"metadata":{"id":"T9SxU5MDhiJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SE Block\n","class SEBlock(nn.Module):\n","    def __init__(self, in_channels, r=4):\n","        super().__init__()\n","\n","        self.squeeze = nn.AdaptiveAvgPool2d((1,1))\n","        self.excitation = nn.Sequential(\n","            nn.Linear(in_channels, in_channels * r),\n","            Swish(),\n","            nn.Linear(in_channels * r, in_channels),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.squeeze(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.excitation(x)\n","        x = x.view(x.size(0), x.size(1), 1, 1)\n","        return x\n","\n","# check\n","if __name__ == '__main__':\n","    x = torch.randn(3, 56, 17, 17)\n","    model = SEBlock(x.size(1))\n","    output = model(x)\n","    print('output size:', output.size())"],"metadata":{"id":"RNTeW2mLhrux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MBConv(nn.Module):\n","    expand = 6\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n","        super().__init__()\n","        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n","\n","        self.residual = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels * MBConv.expand, 1, stride=stride, padding=0, bias=False),\n","            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n","            Swish(),\n","            nn.Conv2d(in_channels * MBConv.expand, in_channels * MBConv.expand, kernel_size=kernel_size,\n","                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*MBConv.expand),\n","            nn.BatchNorm2d(in_channels * MBConv.expand, momentum=0.99, eps=1e-3),\n","            Swish()\n","        )\n","\n","        self.se = SEBlock(in_channels * MBConv.expand, se_scale)\n","\n","        self.project = nn.Sequential(\n","            nn.Conv2d(in_channels*MBConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n","        )\n","\n","        self.shortcut = (stride == 1) and (in_channels == out_channels)\n","\n","    def forward(self, x):\n","        # stochastic depth\n","        if self.training:\n","            if not torch.bernoulli(self.p):\n","                return x\n","\n","        x_shortcut = x\n","        x_residual = self.residual(x)\n","        x_se = self.se(x_residual)\n","\n","        x = x_se * x_residual\n","        x = self.project(x)\n","\n","        if self.shortcut:\n","            x= x_shortcut + x\n","\n","        return x\n","\n","# check\n","if __name__ == '__main__':\n","    x = torch.randn(3, 16, 24, 24)\n","    model = MBConv(x.size(1), x.size(1), 3, stride=1, p=1)\n","    model.train()\n","    output = model(x)\n","    x = (output == x)\n","    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"],"metadata":{"id":"2UYvrXy0kXPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SepConv(nn.Module):\n","    expand = 1\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, se_scale=4, p=0.5):\n","        super().__init__()\n","        # first SepConv is not using stochastic depth\n","        self.p = torch.tensor(p).float() if (in_channels == out_channels) else torch.tensor(1).float()\n","\n","        self.residual = nn.Sequential(\n","            nn.Conv2d(in_channels * SepConv.expand, in_channels * SepConv.expand, kernel_size=kernel_size,\n","                      stride=1, padding=kernel_size//2, bias=False, groups=in_channels*SepConv.expand),\n","            nn.BatchNorm2d(in_channels * SepConv.expand, momentum=0.99, eps=1e-3),\n","            Swish()\n","        )\n","\n","        self.se = SEBlock(in_channels * SepConv.expand, se_scale)\n","\n","        self.project = nn.Sequential(\n","            nn.Conv2d(in_channels*SepConv.expand, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(out_channels, momentum=0.99, eps=1e-3)\n","        )\n","\n","        self.shortcut = (stride == 1) and (in_channels == out_channels)\n","\n","    def forward(self, x):\n","        # stochastic depth\n","        if self.training:\n","            if not torch.bernoulli(self.p):\n","                return x\n","\n","        x_shortcut = x\n","        x_residual = self.residual(x)\n","        x_se = self.se(x_residual)\n","\n","        x = x_se * x_residual\n","        x = self.project(x)\n","\n","        if self.shortcut:\n","            x= x_shortcut + x\n","\n","        return x\n","\n","# check\n","if __name__ == '__main__':\n","    x = torch.randn(3, 16, 24, 24)\n","    model = SepConv(x.size(1), x.size(1), 3, stride=1, p=1)\n","    model.train()\n","    output = model(x)\n","    # stochastic depth check\n","    x = (output == x)\n","    print('output size:', output.size(), 'Stochastic depth:', x[1,0,0,0])"],"metadata":{"id":"BNFkAmEnkXRf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EfficientNet(nn.Module):\n","    def __init__(self, num_classes=10, width_coef=1., depth_coef=1., scale=1., dropout=0.2, se_scale=4, stochastic_depth=False, p=0.5):\n","        super().__init__()\n","        channels = [32, 16, 24, 40, 80, 112, 192, 320, 1280]\n","        repeats = [1, 2, 2, 3, 3, 4, 1]\n","        strides = [1, 2, 2, 2, 1, 2, 1]\n","        kernel_size = [3, 3, 5, 3, 5, 5, 3]\n","        depth = depth_coef\n","        width = width_coef\n","\n","        channels = [int(x*width) for x in channels]\n","        repeats = [int(x*depth) for x in repeats]\n","\n","        # stochastic depth\n","        if stochastic_depth:\n","            self.p = p\n","            self.step = (1 - 0.5) / (sum(repeats) - 1)\n","        else:\n","            self.p = 1\n","            self.step = 0\n","\n","\n","        # efficient net\n","        self.upsample = nn.Upsample(scale_factor=scale, mode='bilinear', align_corners=False)\n","\n","        self.stage1 = nn.Sequential(\n","            nn.Conv2d(3, channels[0],3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(channels[0], momentum=0.99, eps=1e-3)\n","        )\n","\n","        self.stage2 = self._make_Block(SepConv, repeats[0], channels[0], channels[1], kernel_size[0], strides[0], se_scale)\n","\n","        self.stage3 = self._make_Block(MBConv, repeats[1], channels[1], channels[2], kernel_size[1], strides[1], se_scale)\n","\n","        self.stage4 = self._make_Block(MBConv, repeats[2], channels[2], channels[3], kernel_size[2], strides[2], se_scale)\n","\n","        self.stage5 = self._make_Block(MBConv, repeats[3], channels[3], channels[4], kernel_size[3], strides[3], se_scale)\n","\n","        self.stage6 = self._make_Block(MBConv, repeats[4], channels[4], channels[5], kernel_size[4], strides[4], se_scale)\n","\n","        self.stage7 = self._make_Block(MBConv, repeats[5], channels[5], channels[6], kernel_size[5], strides[5], se_scale)\n","\n","        self.stage8 = self._make_Block(MBConv, repeats[6], channels[6], channels[7], kernel_size[6], strides[6], se_scale)\n","\n","        self.stage9 = nn.Sequential(\n","            nn.Conv2d(channels[7], channels[8], 1, stride=1, bias=False),\n","            nn.BatchNorm2d(channels[8], momentum=0.99, eps=1e-3),\n","            Swish()\n","        ) \n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.linear = nn.Linear(channels[8], num_classes)\n","\n","    def forward(self, x):\n","        x = self.upsample(x)\n","        x = self.stage1(x)\n","        x = self.stage2(x)\n","        x = self.stage3(x)\n","        x = self.stage4(x)\n","        x = self.stage5(x)\n","        x = self.stage6(x)\n","        x = self.stage7(x)\n","        x = self.stage8(x)\n","        x = self.stage9(x)\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.dropout(x)\n","        x = self.linear(x)\n","        return x\n","\n","\n","    def _make_Block(self, block, repeats, in_channels, out_channels, kernel_size, stride, se_scale):\n","        strides = [stride] + [1] * (repeats - 1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(in_channels, out_channels, kernel_size, stride, se_scale, self.p))\n","            in_channels = out_channels\n","            self.p -= self.step\n","\n","        return nn.Sequential(*layers)\n","\n","\n","def efficientnet_b0(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.0, scale=1.0,dropout=0.2, se_scale=4)\n","\n","def efficientnet_b1(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.0, depth_coef=1.1, scale=240/224, dropout=0.2, se_scale=4)\n","\n","def efficientnet_b2(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.1, depth_coef=1.2, scale=260/224., dropout=0.3, se_scale=4)\n","\n","def efficientnet_b3(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.2, depth_coef=1.4, scale=300/224, dropout=0.3, se_scale=4)\n","\n","def efficientnet_b4(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.4, depth_coef=1.8, scale=380/224, dropout=0.4, se_scale=4)\n","\n","def efficientnet_b5(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.6, depth_coef=2.2, scale=456/224, dropout=0.4, se_scale=4)\n","\n","def efficientnet_b6(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=1.8, depth_coef=2.6, scale=528/224, dropout=0.5, se_scale=4)\n","\n","def efficientnet_b7(num_classes=2):\n","    return EfficientNet(num_classes=num_classes, width_coef=2.0, depth_coef=3.1, scale=600/224, dropout=0.5, se_scale=4)\n","\n","\n","# check\n","if __name__ == '__main__':\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    x = torch.randn(3, 3, 224, 224).to(device)\n","    model = efficientnet_b0().to(device)\n","    output = model(x)\n","    print('output size:', output.size())"],"metadata":{"id":"FCMYCi-tkXTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print model summary\n","model = efficientnet_b0().to(device)\n","summary(model, (3,224,224), device=device.type)"],"metadata":{"id":"HGujIMEWkjkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print model summary\n","model = efficientnet_b0().to(device)\n","summary(model, (3,224,224), device=device.type)"],"metadata":{"id":"C5w28OsooiUD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define loss function, optimizer, lr_scheduler\n","loss_func = nn.CrossEntropyLoss(reduction='sum')\n","opt = optim.Adam(model.parameters(), lr=0.01)\n","\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=10)\n","\n","\n","# get current lr\n","def get_lr(opt):\n","    for param_group in opt.param_groups:\n","        return param_group['lr']\n","\n","\n","# calculate the metric per mini-batch\n","def metric_batch(output, target):\n","    pred = output.argmax(1, keepdim=True)\n","    corrects = pred.eq(target.view_as(pred)).sum().item()\n","    return corrects\n","\n","\n","# calculate the loss per mini-batch\n","def loss_batch(loss_func, output, target, opt=None):\n","    loss_b = loss_func(output, target)\n","    metric_b = metric_batch(output, target)\n","\n","    if opt is not None:\n","        opt.zero_grad()\n","        loss_b.backward()\n","        opt.step()\n","    \n","    return loss_b.item(), metric_b\n","\n","\n","# calculate the loss per epochs\n","def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n","    running_loss = 0.0\n","    running_metric = 0.0\n","    len_data = len(dataset_dl.dataset)\n","\n","    for xb, yb in dataset_dl:\n","        xb = xb.to(device)\n","        yb = yb.to(device)\n","        output = model(xb)\n","\n","        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n","\n","        running_loss += loss_b\n","        \n","        if metric_b is not None:\n","            running_metric += metric_b\n","\n","        if sanity_check is True:\n","            break\n","\n","    loss = running_loss / len_data\n","    metric = running_metric / len_data\n","    return loss, metric\n","\n","\n","# function to start training\n","def train_val(model, params):\n","    num_epochs=params['num_epochs']\n","    loss_func=params['loss_func']\n","    opt=params['optimizer']\n","    train_dl=params['train_dl']\n","    val_dl=params['val_dl']\n","    sanity_check=params['sanity_check']\n","    lr_scheduler=params['lr_scheduler']\n","    path2weights=params['path2weights']\n","\n","    loss_history = {'train': [], 'val': []}\n","    metric_history = {'train': [], 'val': []}\n","\n","    best_loss = float('inf')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    start_time = time.time()\n","\n","    for epoch in range(num_epochs):\n","        current_lr = get_lr(opt)\n","        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n","\n","        model.train()\n","        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n","        loss_history['train'].append(train_loss)\n","        metric_history['train'].append(train_metric)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n","        loss_history['val'].append(val_loss)\n","        metric_history['val'].append(val_metric)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            torch.save(model.state_dict(), path2weights)\n","            print('Copied best model weights!')\n","\n","        lr_scheduler.step(val_loss)\n","        if current_lr != get_lr(opt):\n","            print('Loading best model weights!')\n","            model.load_state_dict(best_model_wts)\n","\n","        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n","        print('-'*10)\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, loss_history, metric_history"],"metadata":{"id":"nXFHjW82ktg3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define the training parameters\n","params_train = {\n","    'num_epochs':100,\n","    'optimizer':opt,\n","    'loss_func':loss_func,\n","    'train_dl':dataloaders['train'],\n","    'val_dl':dataloaders['valid'],\n","    'sanity_check':False,\n","    'lr_scheduler':lr_scheduler,\n","    'path2weights':'./models/weights.pt',\n","}\n","\n","# check the directory to save weights.pt\n","def createFolder(directory):\n","    try:\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","    except OSerror:\n","        print('Error')\n","createFolder('./models')"],"metadata":{"id":"48nptxtTkti6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, loss_hist, metric_hist = train_val(model, params_train)"],"metadata":{"id":"xOPjbJLomLh1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lOPtNVw1mc5g"},"execution_count":null,"outputs":[]}]}